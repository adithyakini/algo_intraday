{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bhavcopy_nseindia2mysql.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1wpeVd-eds_hCQW83PZg9epPy9KzIGStl",
      "authorship_tag": "ABX9TyMJB6mtlwMxFfurfjI65p8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyakini/algo_intraday/blob/master/bhavcopy_nseindia2mysql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGXTBbodmqb-"
      },
      "source": [
        "pip install mysql.connector, eventlet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm1fi2ahE6g-",
        "outputId": "965368ca-4c8d-479c-e3d1-bf03b6992951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import requests, zipfile, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from progressbar import ProgressBar\n",
        "from termcolor import colored, cprint\n",
        "import time\n",
        "\n",
        "#https://honingds.com/blog/pandas-read_csv/#indexcol\n",
        "\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "dmonth={'01':'JAN','02':'FEB','03':'MAR','04':'APR','05':'MAY','06':'JUN','07':'JUL','08':'AUG','09':'SEP','10':'OCT','11':'NOV','12':'DEC'}\n",
        "holiday = ['2020-04-02','2020-04-06','2020-04-10','2020-04-14','2020-05-01','2020-05-25','2020-10-02','2020-11-16','2020-11-30','2020-12-25']\n",
        "# Before running this script , file called bhavcopy_date.txt need to be present in the \"base\" path.\n",
        "# Opening file named bhavcopy_date.txt , it keeps track of the last downloaded date.\n",
        "ltdl = open(base+'bhavcopy_date.txt','r')\n",
        "lastdt = ltdl.read(10)\n",
        "ltdl.close()\n",
        "lastdt = datetime.strptime(lastdt,'%Y-%m-%d')\n",
        "diff, wr = today-lastdt.date(), ''\n",
        "\n",
        "for i in range(1,diff.days+1): #loop through all dates from the last date mentioned in the bhavcopy_date file until today.\n",
        "  nextdt = lastdt+ relativedelta(days=i) #calculate the next day value\n",
        "  #check if the date is a weekend or market holiday so that we can remove that from the loop , bhavcopies are not available for weekends.\n",
        "  if (nextdt.weekday() == 5 or nextdt.weekday() == 6):\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a weekend','grey')\n",
        "  elif nextdt.strftime('%Y-%m-%d') in holiday:\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a Market Holiday','grey')\n",
        "  else:\n",
        "    d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year #extract day , month and year from  he date\n",
        "    zpath = base+y+'/'+d+'.zip'\n",
        "    if not os.path.isdir(base+y):#if there is no directory already present at the path with the year as a folder then create it\n",
        "      os.mkdir(base+y)\n",
        "      os.mkdir(base+y+'/Index')\n",
        "      os.mkdir(base+y+'/Futures')\n",
        "      os.mkdir(base+y+'/Forex')\n",
        "      os.mkdir(base+y+'/bhavcopy')\n",
        "    equities_bhavcopy=requests.get('https://archives.nseindia.com/products/content/sec_bhavdata_full_'+d+m+y+'.csv', timeout=10)  \n",
        "    if equities_bhavcopy.status_code==200:#if the connection is successful\n",
        "      open('sec_bhavdata_full_'+d+m+y+'.csv', 'wb').write(equities_bhavcopy.content)\n",
        "      #reading and storing in disctionary because we need 2 columns from the MTO file deliverable and %deliverable which is not found in the bhavcopy.\n",
        "      f, deldict = pd.read_csv('sec_bhavdata_full_'+d+m+y+'.csv'), {}  #reading the raw dl-ed bhav file\n",
        "      f = trim(pd.read_csv('sec_bhavdata_full_'+d+m+y+'.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "      f = f[f[' SERIES'] == 'EQ'] #retaining only EQ rows and leaving out bonds,options etc\n",
        "      #cprint('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS', 'red', attrs=['blink'])\n",
        "      print('sec_bhavdata_full_'+d+m+y+'.csv download...SUCCESS')\n",
        "      #write a new csv, bhavcopydate as a column in the csv and get rid of the downloaded file\n",
        "      f['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(f))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "      f = f[['SYMBOL', 'BHAVCOPYDATE', ' OPEN_PRICE', ' HIGH_PRICE', ' LOW_PRICE', ' CLOSE_PRICE', ' TTL_TRD_QNTY', ' NO_OF_TRADES',' DELIV_QTY',' DELIV_PER']]\n",
        "      f = f.rename(columns={'SYMBOL':'SYMBOL', ' OPEN_PRICE':'OPEN', ' HIGH_PRICE':'HIGH', ' LOW_PRICE':'LOW', ' TTL_TRD_QNTY':'TOTTRDQTY', ' NO_OF_TRADES':'NO_OF_TRADES',' DELIV_QTY':'DELIV_QTY',' DELIV_PER':'DELIV_PER'}) #rename some of the columns to something that is easier to underastand\n",
        "      f.to_csv(base+y+'/bhavcopy/'+str(nextdt.date())+'.csv', index=False)\n",
        "          \n",
        "      ######################################################################################\n",
        "      # connect to MySQL db in https://johnny.heliohost.org:2083/ UN \n",
        "      # pip install pip install mysql-connector --target=$nb_path pip install mysql-connector\n",
        "      # https://pynative.com/python-mysql-database-connection/\n",
        "      d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "      #check if the path exist and connect to cloud mysql \n",
        "      #if os.path.exists(base+y):\n",
        "      try:\n",
        "        connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                            database='akini_algotrade',\n",
        "                                            user='akini',\n",
        "                                            password='Drink7up@home')\n",
        "        \n",
        "        db_Info = connection.get_server_info()\n",
        "        print(\"\\nConnected to MySQL Server - version\", db_Info)\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(\"select database();\")\n",
        "        record = cursor.fetchone()\n",
        "        print(\"\\nIngesting EQUITIES bhavcopy \"+'/'+y+'-'+m+'-'+d+'.csv'+\" into DB....:\", record)\n",
        "        cursor.fast_executemany = True\n",
        "        with open(base+y+'/bhavcopy/'+y+'-'+m+'-'+d+'.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "          csvdata = csv.reader(csvfile)\n",
        "          #skip the 1st row as it will be header\n",
        "          next(csvdata)\n",
        "          pbar = ProgressBar()\n",
        "          for row in pbar(list(csvdata)):\n",
        "            # Prepare SQL query to INSERT a record into the database.\n",
        "            sql_stocks = \"INSERT INTO bhavcopy (symbol, bhavcopydate, open, high, low, close, tottrdqty, no_of_trades, delivery, delivery_percent) \\\n",
        "            VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9])\n",
        "            #print(sql)\n",
        "            try:\n",
        "              #Execute the SQL command\n",
        "              cursor.execute(sql_stocks)\n",
        "              #Commit your changes in the database\n",
        "              connection.commit()\n",
        "            except Error as e:\n",
        "              cprint('-------------------------------','red')\n",
        "              print(\"Error while connecting to MySQL\", e)\n",
        "              cprint('-------------------------------','red')\n",
        "              connection.rollback()\n",
        "              #pbar.next()\n",
        "              pbar.finish()\n",
        "              break\n",
        "      except Error as e:\n",
        "        cprint('-------------------------------','red')\n",
        "        print(\"Error while connecting to MySQL\", e)\n",
        "        cprint('-------------------------------','red')\n",
        "        break\n",
        "      finally:\n",
        "        if (connection.is_connected()):\n",
        "          cursor.close()\n",
        "          connection.close()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sec_bhavdata_full_25092020.csv download...SUCCESS\n",
            "\u001b[31m-------------------------------\u001b[0m\n",
            "Error while connecting to MySQL 2003: Can't connect to MySQL server on 'johnny.heliohost.org:3306' (111 Connection refused)\n",
            "\u001b[31m-------------------------------\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQ7pBunZZyl",
        "outputId": "5fb2adcc-bccf-4234-b811-d44df832a117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "import requests, zipfile, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from progressbar import ProgressBar\n",
        "from termcolor import colored, cprint\n",
        "import time\n",
        "\n",
        "#https://honingds.com/blog/pandas-read_csv/#indexcol\n",
        "\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "dmonth={'01':'JAN','02':'FEB','03':'MAR','04':'APR','05':'MAY','06':'JUN','07':'JUL','08':'AUG','09':'SEP','10':'OCT','11':'NOV','12':'DEC'}\n",
        "holiday = ['2020-04-02','2020-04-06','2020-04-10','2020-04-14','2020-05-01','2020-05-25','2020-10-02','2020-11-16','2020-11-30','2020-12-25']\n",
        "# Before running this script , file called bhavcopy_date.txt need to be present in the \"base\" path.\n",
        "# Opening file named bhavcopy_date.txt , it keeps track of the last downloaded date.\n",
        "ltdl = open(base+'bhavcopy_date.txt','r')\n",
        "lastdt = ltdl.read(10)\n",
        "ltdl.close()\n",
        "lastdt = datetime.strptime(lastdt,'%Y-%m-%d')\n",
        "diff, wr = today-lastdt.date(), ''\n",
        "\n",
        "for i in range(1,diff.days+1): #loop through all dates from the last date mentioned in the bhavcopy_date file until today.\n",
        "  nextdt = lastdt+ relativedelta(days=i) #calculate the next day value\n",
        "  #check if the date is a weekend or market holiday so that we can remove that from the loop , bhavcopies are not available for weekends.\n",
        "  if (nextdt.weekday() == 5 or nextdt.weekday() == 6):\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a weekend','grey')\n",
        "  elif nextdt.strftime('%Y-%m-%d') in holiday:\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a Market Holiday','grey')\n",
        "  else:\n",
        "    d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year #extract day , month and year from  he date\n",
        "    zpath = base+y+'/'+d+'.zip'\n",
        "    if not os.path.isdir(base+y):#if there is no directory already present at the path with the year as a folder then create it\n",
        "      os.mkdir(base+y)\n",
        "      os.mkdir(base+y+'/Index')\n",
        "      os.mkdir(base+y+'/Futures')\n",
        "      os.mkdir(base+y+'/Forex')\n",
        "      os.mkdir(base+y+'/bhavcopy')\n",
        "    for i in range(3): #try to connect to the nseindia url to download the bhavcopy , 3 times , just incase website does not respond etc.\n",
        "      while True:\n",
        "        try:\n",
        "          equities_bhavcopy=requests.get('https://archives.nseindia.com/content/historical/EQUITIES/'+y+'/'+dmonth[m]+'/cm'+d+dmonth[m]+y+'bhav.csv.zip')\n",
        "        except requests.ConnectionError:\n",
        "          print('No connection, retrying')\n",
        "        break\n",
        "    if equities_bhavcopy.status_code==200:#if the connection is successful\n",
        "      dload=open(zpath, 'wb')\n",
        "      dload.write(equities_bhavcopy.content)\n",
        "      dload.close()\n",
        "      #open the downlaoded bhavcopy and extract it\n",
        "      z = zipfile.ZipFile(zpath, 'r')\n",
        "      z.extractall(base+y+'/bhavcopy')\n",
        "      z.close()\n",
        "      os.remove(zpath)\n",
        "      #reading and storing in 2 dictionaries because we need 2 columns from the MTO file deliverable and %deliverable which is not found in the bhavcopy.\n",
        "      f, deldict = pd.read_csv(base+y+'/bhavcopy/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "      f, deldict2 = pd.read_csv(base+y+'/bhavcopy/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "      f = f[f['SERIES'] == 'EQ'] #retaining only EQ rows and leaving out bonds,options etc\n",
        "      #cprint('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS', 'red', attrs=['blink'])\n",
        "      print('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS')\n",
        "      # a file called mto.dat holds deliverable data, it is useful data to know whether the delivery percentage of a stock has gone up , an indication that long term investors have gone up\n",
        "      for i in range(7): #try to connect to the nseindia url to download the mto file , 7 times , just incase website does not respond etc.\n",
        "        while True:\n",
        "          try:\n",
        "            deliverable = requests.get('https://archives.nseindia.com/archives/equities/mto/MTO_'+d+m+y+'.DAT').text.splitlines()\n",
        "          except requests.ConnectionError:\n",
        "            cprint('No connection, retrying','red')\n",
        "          break\n",
        "      print(''+d+m+y+'.DAT download ...SUCCESS')\n",
        "      del deliverable[:4]\n",
        "      for i in deliverable:\n",
        "          c = i.split(',')\n",
        "          if c[3] == 'EQ' :                \n",
        "              deldict[c[2]] = c[5] #building delivarables dict\n",
        "          if c[3] == 'EQ' :                \n",
        "              deldict2[c[2]] = c[6] #building %delivarables dict\n",
        "      dfdel = pd.DataFrame(list(deldict.items()), columns = ['SYMBOL', 'DELIVERABLE'])\n",
        "      dfdel2 = pd.DataFrame(list(deldict2.items()), columns = ['SYMBOL', '%DELIVERABLE'])\n",
        "      f = f.merge(dfdel, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "      f = f.merge(dfdel2, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "      #write a new csv, bhavcopydate as a column in the csv and get rid of the downloaded file\n",
        "      f['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(f))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "      f = f[['SYMBOL', 'BHAVCOPYDATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'TOTTRDQTY', 'DELIVERABLE','%DELIVERABLE']]\n",
        "      \n",
        "      f.to_csv(base+y+'/bhavcopy/'+str(nextdt.date())+'.csv', index=False)\n",
        "      os.remove(base+y+'/bhavcopy/cm'+d+dmonth[m]+y+'bhav.csv')\n",
        "      print('Merging of deliverable column to EQ bhavcopy...SUCCESS')\n",
        "      \n",
        "      ######################################################################################\n",
        "      # connect to MySQL db in https://johnny.heliohost.org:2083/ UN \n",
        "      # pip install pip install mysql-connector --target=$nb_path pip install mysql-connector\n",
        "      # https://pynative.com/python-mysql-database-connection/\n",
        "      d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "      #check if the path exist and connect to cloud mysql \n",
        "      #if os.path.exists(base+y):\n",
        "      try:\n",
        "        connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                            database='akini_algotrade',\n",
        "                                            user='akini',\n",
        "                                            password='Drink7up@home')\n",
        "        \n",
        "        db_Info = connection.get_server_info()\n",
        "        print(\"\\nConnected to MySQL Server - version\", db_Info)\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(\"select database();\")\n",
        "        record = cursor.fetchone()\n",
        "        print(\"\\nIngesting EQUITIES bhavcopy \"+'/'+y+'-'+m+'-'+d+'.csv'+\" into DB....:\", record)\n",
        "        cursor.fast_executemany = True\n",
        "        with open(base+y+'/bhavcopy/'+y+'-'+m+'-'+d+'.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "          csvdata = csv.reader(csvfile)\n",
        "          #skip the 1st row as it will be header\n",
        "          next(csvdata)\n",
        "          pbar = ProgressBar()\n",
        "          for row in pbar(list(csvdata)):\n",
        "            # Prepare SQL query to INSERT a record into the database.\n",
        "            sql_stocks = \"INSERT INTO bhavcopy (symbol, bhavcopydate, open, high, low, close, tottrdqty, deliverable, deliverable_percent) \\\n",
        "            VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8])\n",
        "            #print(sql)\n",
        "            try:\n",
        "              #Execute the SQL command\n",
        "              cursor.execute(sql_stocks)\n",
        "              #Commit your changes in the database\n",
        "              connection.commit()\n",
        "            except Error as e:\n",
        "              cprint('-------------------------------','red')\n",
        "              print(\"Error while connecting to MySQL\", e)\n",
        "              cprint('-------------------------------','red')\n",
        "              connection.rollback()\n",
        "              #pbar.next()\n",
        "              pbar.finish()\n",
        "              break\n",
        "      except Error as e:\n",
        "        cprint('-------------------------------','red')\n",
        "        print(\"Error while connecting to MySQL\", e)\n",
        "        cprint('-------------------------------','red')\n",
        "        break\n",
        "      finally:\n",
        "        if (connection.is_connected()):\n",
        "          cursor.close()\n",
        "          connection.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #now lets work with the FOREX\n",
        "    for i in range(2): #try to connect to the nseindia url to download the bhavcopy , 3 times , just incase website does not respond etc.\n",
        "      #while True:\n",
        "      try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',}#Sometimes requests from requests.get() gets blocked by server, so solution is to make the server think the request is coming from a web browser\n",
        "        forex = requests.get('https://archives.nseindia.com/archives/cd/mkt_act/cd'+d+m+y+'.zip', headers=headers, timeout=5) #get the zip file from nseindia\n",
        "      except requests.exceptions.Timeout as error:\n",
        "        cprint('ERROR!!! 404 filenotfound cd'+d+m+y+'.zip..skipping','red')\n",
        "        break\n",
        "      except requests.ConnectionError:\n",
        "        cprint('No connection.. retrying','red')\n",
        "        break\n",
        "    if forex.status_code==200:\n",
        "        fx=open(zpath, 'wb') #open the zip file under a temp location called zpath that is defined above\n",
        "        fx.write(forex.content)\n",
        "        fx.close()\n",
        "        z, wr = zipfile.ZipFile(zpath,'r'), nextdt.date()\n",
        "        z.extractall(base+y+'/Forex') #extract the contents of the zip file to a location called forex , it extracts some 6 csv files we want only 1\n",
        "        z.close()\n",
        "        os.remove(zpath) #remove the zip file\n",
        "        if os.path.exists(base+y+'/Forex/cf'+d+m+y+'.csv'):\n",
        "          def trim(dataset): #Definition for strippping whitespace\n",
        "              trim = lambda x: x.strip() if type(x) is str else x\n",
        "              return dataset.applymap(trim)\n",
        "          # making dataframe from csv file\n",
        "          data = trim(pd.read_csv(base+y+'/Forex/cf'+d+m+y+'.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "          data = data[data['INSTRUMENT'] == 'FUTCUR'] #retaining only FUTCUR rows and leaving out other rows\n",
        "          data[['INSTRUMENT', 'SYMBOL    ', 'EXP_DATE  ', 'OPEN_PRICE ', 'HI_PRICE   ', 'LO_PRICE   ', 'CLOSE_PRICE','OPEN_INT*      ','TRD_VAL           ','TRD_QTY          ','NO_OF_CONT       ','NO_OF_TRADE      ']]#the useless file has spaces in the heading.\n",
        "          data = data.rename(columns={'SYMBOL    ':'SYMBOL', 'EXP_DATE  ':'EXP_DATE', 'OPEN_PRICE ':'OPEN', 'HI_PRICE   ':'HIGH', 'LO_PRICE   ':'LOW', 'CLOSE_PRICE':'CLOSE','OPEN_INT*      ':'OPEN_INT','TRD_VAL           ':'TRD_VAL','TRD_QTY          ':'TRD_QTY','NO_OF_CONT       ':'NO_OF_CONT','NO_OF_TRADE      ':'NO_OF_TRADE'}) #rename some of the columns to something that is easier to underastand\n",
        "          data['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(data))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "          data['EXP_DATE'] = pd.to_datetime(data['EXP_DATE'], format = '%d/%m/%Y') #the exp_date column is in format dd/mm/yyy convert it to yyyy-mm-dd\n",
        "          data.to_csv(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', index=False) #write the changes above to a new file and add an _forex to the csv file\n",
        "          os.remove(base+y+'/Forex/cf'+d+m+y+'.csv')\n",
        "          print('Forex bhavcopy '+ str(nextdt.date())+'_forex.csv create...SUCCESS')\n",
        "          #print(data)\n",
        "          #cleanup all the useless files that get extracted from the currency futures bhavcopy\n",
        "          for CleanUp in glob.glob(base+y+'/Forex/*.*'): # list out the files\n",
        "            if not re.match(\".+forex+\",CleanUp): #if the list found above contains \"forex\" then dont do anything, else delete\n",
        "              os.remove(CleanUp) #remove files in the folder\n",
        "            \n",
        "          try:\n",
        "            connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                                database='akini_algotrade',\n",
        "                                                user='akini',\n",
        "                                                password='Drink7up@home')\n",
        "            \n",
        "            db_Info = connection.get_server_info()\n",
        "            print(\"\\nConnected to MySQL Server version\", db_Info)\n",
        "            #print(\">>>>>>>>>>>>>>>\\n\")\n",
        "            cursor = connection.cursor()\n",
        "            cursor.execute(\"select database();\")\n",
        "            record = cursor.fetchone()\n",
        "            print(\"\\nIngesting FOREX bhavcopy \"+'/'+y+'-'+m+'-'+d+'_forex.csv'+\" into DB....:\", record)\n",
        "            #print(\">>>>>>>>>>>>>>>\\n\")\n",
        "            cursor.fast_executemany = True\n",
        "            with open(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', newline='',  encoding=\"utf8\") as csvfile_forex:\n",
        "              csvdata_forex = csv.reader(csvfile_forex)\n",
        "              #skip the 1st row as it will be header\n",
        "              next(csvdata_forex)\n",
        "              pbar = ProgressBar()\n",
        "              for row in pbar(list(csvdata_forex)):\n",
        "              #Prepare SQL query to INSERT a record into the database.\n",
        "                sql_forex = \"INSERT INTO forex (symbol, exp_date, open, high, low, close, open_int, trd_val, trd_qty, no_of_cont, no_of_trade, bhavcopydate) \\\n",
        "                VALUES ('%s','%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s', '%s','%s');\" % (row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11], row[12])\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_forex)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "          except Error as e:\n",
        "            cprint('-------------------------------','red')\n",
        "            print(\"Error while connecting to MySQL\", e)\n",
        "            cprint('-------------------------------','red')\n",
        "            break\n",
        "          finally:\n",
        "            # create cursor\n",
        "            cursor=connection.cursor()\n",
        "            csvfile = requests.get('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/edit#gid=0')\n",
        "            print(csvfile.status_code)\n",
        "            print('Updating Banned Symbols table')\n",
        "            # Insert DataFrame recrds one by one.\n",
        "            df=pd.read_csv('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/export?format=csv&gid=0', usecols=[\"Stocks not allowed for MIS\"])\n",
        "            pbar = ProgressBar()\n",
        "            #Prepare SQL query to INSERT a record into the database.\n",
        "            if csvfile.status_code != 200:\n",
        "              #truncate banned symbols table\n",
        "              sql_truncate_table = \"truncate banned_symbols;\"\n",
        "              cursor.execute(sql_truncate_table)\n",
        "              connection.commit()\n",
        "              time.sleep(10)\n",
        "              for i,row in pbar(df.iterrows()):\n",
        "                sql_banned_symbols = \"INSERT INTO banned_symbols (symbol) VALUES ('%s');\" % (row[0])\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_banned_symbols)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "                  connection.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #now lets work with the FUTURES\n",
        "    for i in range(2): #try to connect to the nseindia url to download the bhavcopy , 3 times , just incase website does not respond etc.\n",
        "      #while True:\n",
        "      try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',}#Sometimes requests from requests.get() gets blocked by server, so solution is to make the server think the request is coming from a web browser\n",
        "        futures_bhavcopy = requests.get('https://archives.nseindia.com/content/historical/DERIVATIVES/'+y+'/'+dmonth[m]+'/fo'+d+dmonth[m]+y+'bhav.csv.zip', headers=headers, timeout=5) #get the zip file from nseindia\n",
        "      except requests.exceptions.Timeout as error:\n",
        "        cprint('ERROR!!! 404 filenotfound fo'+d+dmonth[m]+y+'bhav.csv.zip..skipping','red')\n",
        "      except requests.ConnectionError:\n",
        "        cprint('No connection.. retrying','red')\n",
        "    if futures_bhavcopy.status_code==200:\n",
        "        dload=open(zpath, 'wb')\n",
        "        dload.write(futures_bhavcopy.content)\n",
        "        dload.close()\n",
        "        #open the downlaoded bhavcopy and extract it\n",
        "        z = zipfile.ZipFile(zpath, 'r')\n",
        "        z.extractall(base+y+'/futures')\n",
        "        z.close()\n",
        "        os.remove(zpath)\n",
        "        #reading and storing in a dictionary\n",
        "        f, deldict = pd.read_csv(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "        #cprint('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS', 'red', attrs=['blink'])\n",
        "        print('fo'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS')\n",
        "        # making dataframe from csv file\n",
        "        if os.path.exists(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'):\n",
        "          def trim(dataset): #Definition for strippping whitespace\n",
        "              trim = lambda x: x.strip() if type(x) is str else x\n",
        "              return dataset.applymap(trim)\n",
        "          # making dataframe from csv file\n",
        "          data = trim(pd.read_csv(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "          df1 = data[data['INSTRUMENT'] == 'FUTIDX'] #dataframe of only FUTIDX rows\n",
        "          df2 = data[data['INSTRUMENT'] == 'FUTSTK'] #dataframe of only FUTSTK rows\n",
        "          data = pd.concat([df1,df2]) #concatennating the 2 dataframes\n",
        "          data=data[['INSTRUMENT', 'SYMBOL', 'EXPIRY_DT', 'OPEN', 'HIGH', 'LOW', 'CLOSE','SETTLE_PR','CONTRACTS','VAL_INLAKH','OPEN_INT','CHG_IN_OI']]\n",
        "          data['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(data))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "          data['EXPIRY_DT'] = pd.to_datetime(data['EXPIRY_DT'], format = '%d-%b-%Y') #the exp_date column is in format dd/mmm/yyyy convert it to yyyy-mm-dd\n",
        "          data.to_csv(base+y+'/futures/'+ str(nextdt.date())+'_futures.csv', index=False) #write the changes above to a new file and add an _futures to the csv file\n",
        "          os.remove(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv')\n",
        "          print('Futures bhavcopy '+ str(nextdt.date())+'_futures.csv create...SUCCESS')\n",
        "          d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "          #check if the path exist and connect to cloud mysql \n",
        "          #if os.path.exists(base+y):\n",
        "          try:\n",
        "            connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                                database='akini_algotrade',\n",
        "                                                user='akini',\n",
        "                                                password='Drink7up@home')\n",
        "            \n",
        "            db_Info = connection.get_server_info()\n",
        "            print(\"\\nConnected to MySQL Server - version\", db_Info)\n",
        "            cursor = connection.cursor()\n",
        "            cursor.execute(\"select database();\")\n",
        "            record = cursor.fetchone()\n",
        "            print(\"\\nIngesting FUTURES bhavcopy \"+'/'+y+'-'+m+'-'+d+'_futures.csv'+\" into DB....:\", record)\n",
        "            cursor.fast_executemany = True\n",
        "            with open(base+y+'/futures/'+y+'-'+m+'-'+d+'_futures.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "              csvdata = csv.reader(csvfile)\n",
        "              #skip the 1st row as it will be header\n",
        "              next(csvdata)\n",
        "              pbar = ProgressBar()\n",
        "              for row in pbar(list(csvdata)):\n",
        "                # Prepare SQL query to INSERT a record into the database.\n",
        "                sql_stocks = \"INSERT INTO futures (instrument,symbol, expiry_dt, open, high, low, close, settle_pr,contracts,val_inlakh,open_int,chg_in_oi,bhavcopydate) \\\n",
        "                VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s', '%s','%s', '%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11], row[12])\n",
        "                #print(sql)\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_stocks)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "          except Error as e:\n",
        "            cprint('-------------------------------','red')\n",
        "            print(\"Error while connecting to MySQL\", e)\n",
        "            cprint('-------------------------------','red')\n",
        "            break\n",
        "          finally:\n",
        "            if (connection.is_connected()):\n",
        "              cursor.close()\n",
        "              connection.close()\n",
        "if (connection.is_connected()):\n",
        "  cursor.close()\n",
        "  connection.close()\n",
        "cprint(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\nDONE - All imports complete\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\",'green')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[30m2020-09-26 is a weekend\u001b[0m\n",
            "\u001b[30m2020-09-27 is a weekend\u001b[0m\n",
            "cm28SEP2020bhav.csv.zip download...SUCCESS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-fd22e880acbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mdeliverable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://archives.nseindia.com/archives/equities/mto/MTO_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.DAT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mcprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No connection, retrying'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpCa4rxbg1lP"
      },
      "source": [
        "import requests, zipfile, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from progressbar import ProgressBar\n",
        "from termcolor import colored, cprint\n",
        "import time\n",
        "\n",
        "#https://honingds.com/blog/pandas-read_csv/#indexcol\n",
        "\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "dmonth={'01':'JAN','02':'FEB','03':'MAR','04':'APR','05':'MAY','06':'JUN','07':'JUL','08':'AUG','09':'SEP','10':'OCT','11':'NOV','12':'DEC'}\n",
        "holiday = ['2020-04-02','2020-04-06','2020-04-10','2020-04-14','2020-05-01','2020-05-25','2020-10-02','2020-11-16','2020-11-30','2020-12-25']\n",
        "# Before running this script , file called bhavcopy_date.txt need to be present in the \"base\" path.\n",
        "# Opening file named bhavcopy_date.txt , it keeps track of the last downloaded date.\n",
        "ltdl = open(base+'bhavcopy_date.txt','r')\n",
        "lastdt = ltdl.read(10)\n",
        "ltdl.close()\n",
        "lastdt = datetime.strptime(lastdt,'%Y-%m-%d')\n",
        "diff, wr = today-lastdt.date(), ''\n",
        "\n",
        "for i in range(1,diff.days+1): #loop through all dates from the last date mentioned in the bhavcopy_date file until today.\n",
        "  nextdt = lastdt+ relativedelta(days=i) #calculate the next day value\n",
        "  #check if the date is a weekend or market holiday so that we can remove that from the loop , bhavcopies are not available for weekends.\n",
        "  if (nextdt.weekday() == 5 or nextdt.weekday() == 6):\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a weekend','grey')\n",
        "  elif nextdt.strftime('%Y-%m-%d') in holiday:\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a Market Holiday','grey')\n",
        "  else:\n",
        "    d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year #extract day , month and year from  he date\n",
        "    zpath = base+y+'/'+d+'.zip'\n",
        "    if not os.path.isdir(base+y):#if there is no directory already present at the path with the year as a folder then create it\n",
        "      os.mkdir(base+y)\n",
        "      os.mkdir(base+y+'/Index')\n",
        "      os.mkdir(base+y+'/Futures')\n",
        "      os.mkdir(base+y+'/Forex')\n",
        "      os.mkdir(base+y+'/bhavcopy')\n",
        "    \n",
        "    #now lets work with the FUTURES\n",
        "    for i in range(2): #try to connect to the nseindia url to download the bhavcopy , 3 times , just incase website does not respond etc.\n",
        "      #while True:\n",
        "      try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',}#Sometimes requests from requests.get() gets blocked by server, so solution is to make the server think the request is coming from a web browser\n",
        "        futures_bhavcopy = requests.get('https://archives.nseindia.com/content/historical/DERIVATIVES/'+y+'/'+dmonth[m]+'/fo'+d+dmonth[m]+y+'bhav.csv.zip', headers=headers, timeout=5) #get the zip file from nseindia\n",
        "      except requests.exceptions.Timeout as error:\n",
        "        cprint('ERROR!!! 404 filenotfound fo'+d+dmonth[m]+y+'bhav.csv.zip..skipping','red')\n",
        "      except requests.ConnectionError:\n",
        "        cprint('No connection.. retrying','red')\n",
        "    if futures_bhavcopy.status_code==200:\n",
        "        dload=open(zpath, 'wb')\n",
        "        dload.write(futures_bhavcopy.content)\n",
        "        dload.close()\n",
        "        #open the downlaoded bhavcopy and extract it\n",
        "        z = zipfile.ZipFile(zpath, 'r')\n",
        "        z.extractall(base+y+'/futures')\n",
        "        z.close()\n",
        "        os.remove(zpath)\n",
        "        #reading and storing in a dictionary\n",
        "        f, deldict = pd.read_csv(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "        #cprint('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS', 'red', attrs=['blink'])\n",
        "        print('fo'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS')\n",
        "        # making dataframe from csv file\n",
        "        if os.path.exists(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'):\n",
        "          def trim(dataset): #Definition for strippping whitespace\n",
        "              trim = lambda x: x.strip() if type(x) is str else x\n",
        "              return dataset.applymap(trim)\n",
        "          # making dataframe from csv file\n",
        "          data = trim(pd.read_csv(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "          df1 = data[data['INSTRUMENT'] == 'FUTIDX'] #dataframe of only FUTIDX rows\n",
        "          df2 = data[data['INSTRUMENT'] == 'FUTSTK'] #dataframe of only FUTSTK rows\n",
        "          data = pd.concat([df1,df2]) #concatennating the 2 dataframes\n",
        "          data=data[['INSTRUMENT', 'SYMBOL', 'EXPIRY_DT', 'OPEN', 'HIGH', 'LOW', 'CLOSE','SETTLE_PR','CONTRACTS','VAL_INLAKH','OPEN_INT','CHG_IN_OI']]\n",
        "          data['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(data))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "          data['EXPIRY_DT'] = pd.to_datetime(data['EXPIRY_DT'], format = '%d-%b-%Y') #the exp_date column is in format dd/mmm/yyyy convert it to yyyy-mm-dd\n",
        "          data.to_csv(base+y+'/futures/'+ str(nextdt.date())+'_futures.csv', index=False) #write the changes above to a new file and add an _futures to the csv file\n",
        "          os.remove(base+y+'/futures/fo'+d+dmonth[m]+y+'bhav.csv')\n",
        "          print('Futures bhavcopy '+ str(nextdt.date())+'_futures.csv create...SUCCESS')\n",
        "                  \n",
        "          ######################################################################################\n",
        "          # connect to MySQL db in https://johnny.heliohost.org:2083/ UN \n",
        "          # pip install pip install mysql-connector --target=$nb_path pip install mysql-connector\n",
        "          # https://pynative.com/python-mysql-database-connection/\n",
        "          d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "          #check if the path exist and connect to cloud mysql \n",
        "          #if os.path.exists(base+y):\n",
        "          try:\n",
        "            connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                                database='akini_algotrade',\n",
        "                                                user='akini',\n",
        "                                                password='Drink7up@home')\n",
        "            \n",
        "            db_Info = connection.get_server_info()\n",
        "            print(\"\\nConnected to MySQL Server - version\", db_Info)\n",
        "            cursor = connection.cursor()\n",
        "            cursor.execute(\"select database();\")\n",
        "            record = cursor.fetchone()\n",
        "            print(\"\\nIngesting FUTURES bhavcopy \"+'/'+y+'-'+m+'-'+d+'_futures.csv'+\" into DB....:\", record)\n",
        "            cursor.fast_executemany = True\n",
        "            with open(base+y+'/futures/'+y+'-'+m+'-'+d+'_futures.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "              csvdata = csv.reader(csvfile)\n",
        "              #skip the 1st row as it will be header\n",
        "              next(csvdata)\n",
        "              pbar = ProgressBar()\n",
        "              for row in pbar(list(csvdata)):\n",
        "                # Prepare SQL query to INSERT a record into the database.\n",
        "                sql_stocks = \"INSERT INTO futures (instrument,symbol, expiry_dt, open, high, low, close, settle_pr,contracts,val_inlakh,open_int,chg_in_oi,bhavcopydate) \\\n",
        "                VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s', '%s','%s', '%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11], row[12])\n",
        "                #print(sql)\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_stocks)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "          except Error as e:\n",
        "            cprint('-------------------------------','red')\n",
        "            print(\"Error while connecting to MySQL\", e)\n",
        "            cprint('-------------------------------','red')\n",
        "            break\n",
        "          finally:\n",
        "            if (connection.is_connected()):\n",
        "              cursor.close()\n",
        "              connection.close()\n",
        "if (connection.is_connected()):\n",
        "  cursor.close()\n",
        "  connection.close()\n",
        "cprint(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\nDONE - All imports complete\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\",'green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvEErVv15gsu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcnb-noyOvgH"
      },
      "source": [
        "#********************************************************\n",
        "# Ingest Banned symbols from zerodha excel sheet into DB#\n",
        "#********************************************************\n",
        "import pandas as pd\n",
        "import requests as requests\n",
        "from progressbar import ProgressBar\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import time\n",
        "\n",
        "# Connect to the database\n",
        "connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                                database='akini_algotrade',\n",
        "                                                user='akini',\n",
        "                                                password='Drink7up@home')\n",
        "# create cursor\n",
        "cursor=connection.cursor()\n",
        "csvfile = requests.get('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/edit#gid=0')\n",
        "print(csvfile.status_code)\n",
        "# Insert DataFrame recrds one by one.\n",
        "df=pd.read_csv('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/export?format=csv&gid=0', usecols=[\"Stocks not allowed for MIS\"])\n",
        "pbar = ProgressBar()\n",
        "#truncate banned symbols table\n",
        "sql_truncate_table = \"truncate banned_symbols;\"\n",
        "cursor.execute(sql_truncate_table)\n",
        "connection.commit()\n",
        "time.sleep(10)\n",
        "#Prepare SQL query to INSERT a record into the database.\n",
        "for i,row in pbar(df.iterrows()):\n",
        "  sql_banned_symbols = \"INSERT INTO banned_symbols (symbol) VALUES ('%s');\" % (row[0])\n",
        "  try:\n",
        "    #Execute the SQL command\n",
        "    cursor.execute(sql_banned_symbols)\n",
        "    #Commit your changes in the database\n",
        "    connection.commit()\n",
        "  except Error as e:\n",
        "    cprint('-------------------------------','red')\n",
        "    print(\"Error while connecting to MySQL\", e)\n",
        "    cprint('-------------------------------','red')\n",
        "    connection.rollback()\n",
        "    #pbar.next()\n",
        "    pbar.finish()\n",
        "    break\n",
        "if (connection.is_connected()):\n",
        "              cursor.close()\n",
        "              connection.close()\n",
        "cprint(\">>>>>>>>>>\\nDONE\\n>>>>>>>>>>\",'green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9eKJeMSoVuU"
      },
      "source": [
        "#********************************************\n",
        "# download and insert sector indices symbols#\n",
        "#********************************************\n",
        "import requests, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import glob\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "from progressbar import ProgressBar\n",
        "from termcolor import colored, cprint\n",
        "import time\n",
        "\n",
        "#https://honingds.com/blog/pandas-read_csv/#indexcol\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "d, m, y = '%02d' % today.day, '%02d' % today.month, '%02d' % today.year #extract day , month and year from  he date\n",
        "\n",
        "if not os.path.isdir(base+y+'/sectors'):#if there is no directory already present at the path with the year as a folder then create it\n",
        "      os.mkdir(base+y+'/sectors')\n",
        "\n",
        "#add a not condition to the below to skip over it.\n",
        "if os.path.exists(base+y+'/sectors'):\n",
        "  for CleanUp in glob.glob(base+y+'/sectors/*.*'): # list out the files\n",
        "    os.remove(CleanUp) #remove files in the folder\n",
        "  #download automobile csv\n",
        "  indices_automobile = requests.get('https://archives.nseindia.com/content/indices/ind_niftyautolist.csv').content\n",
        "  aut_csv = pd.read_csv(io.StringIO(indices_automobile.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  aut_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_automobile.csv', index=False)\n",
        "  #download Bank csv\n",
        "  indices_bank = requests.get('https://archives.nseindia.com/content/indices/ind_niftybanklist.csv').content\n",
        "  sec_bank_csv = pd.read_csv(io.StringIO(indices_bank.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_bank_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_bank.csv', index=False)\n",
        "  #download sec_consumer_durable csv\n",
        "  indices_consumer_durable = requests.get('https://archives.nseindia.com/content/indices/ind_niftyconsumerdurableslist.csv').content\n",
        "  sec_consumer_durable_csv = pd.read_csv(io.StringIO(indices_consumer_durable.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_consumer_durable_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_consumer_durable.csv', index=False)\n",
        "  #download financial services csv\n",
        "  indices_fin_serv = requests.get('https://archives.nseindia.com/content/indices/ind_niftyfinancelist.csv').content\n",
        "  sec_fin_serv_csv = pd.read_csv(io.StringIO(indices_fin_serv.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_fin_serv_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_fin_serv.csv', index=False)\n",
        "  #download fmcg csv\n",
        "  indices_fmcg = requests.get('https://archives.nseindia.com/content/indices/ind_niftyfmcglist.csv').content\n",
        "  sec_fmcg_csv = pd.read_csv(io.StringIO(indices_fmcg.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_fmcg_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_fmcg.csv', index=False)\n",
        "  #download IT csv\n",
        "  indices_it = requests.get('https://archives.nseindia.com/content/indices/ind_niftyitlist.csv').content\n",
        "  sec_it_csv = pd.read_csv(io.StringIO(indices_it.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_it_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_it.csv', index=False)\n",
        "  #download media csv\n",
        "  indices_media = requests.get('https://archives.nseindia.com/content/indices/ind_niftymedialist.csv').content\n",
        "  sec_media_csv = pd.read_csv(io.StringIO(indices_media.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_media_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_media.csv', index=False)\n",
        "  #download metal csv\n",
        "  indices_metal = requests.get('https://archives.nseindia.com/content/indices/ind_niftymetallist.csv').content\n",
        "  sec_metal_csv = pd.read_csv(io.StringIO(indices_metal.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_metal_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_metal.csv', index=False)\n",
        "  #download oil_gas csv\n",
        "  indices_oil_gas = requests.get('https://www1.nseindia.com/content/indices/ind_niftyoilgaslist.csv').content\n",
        "  sec_oil_gas_csv = pd.read_csv(io.StringIO(indices_oil_gas.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_oil_gas_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_oil_gas.csv', index=False)\n",
        "  #download pharma csv\n",
        "  indices_pharma = requests.get('https://archives.nseindia.com/content/indices/ind_niftypharmalist.csv').content\n",
        "  sec_pharma_csv = pd.read_csv(io.StringIO(indices_pharma.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_pharma_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_pharma.csv', index=False)\n",
        "  #download realty csv\n",
        "  indices_realty = requests.get('https://archives.nseindia.com/content/indices/ind_niftyrealtylist.csv').content\n",
        "  sec_realty_csv = pd.read_csv(io.StringIO(indices_realty.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "  sec_realty_csv.to_csv(base+y+'/sectors/'+y+'-'+m+'-'+d+'_realty.csv', index=False)\n",
        "\n",
        "#truncate sector_indice_symbols table\n",
        "sql_truncate_table = \"truncate sector_indice_symbols;\"\n",
        "cursor.execute(sql_truncate_table)\n",
        "connection.commit()\n",
        "time.sleep(10)\n",
        "#list out the files in the sector folder\n",
        "for files in glob.glob(base+y+'/sectors/*.*'):\n",
        "  print('\\nIngesting sectoral CSV '+files)\n",
        "  try:\n",
        "    connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                        database='akini_algotrade',\n",
        "                                        user='akini',\n",
        "                                        password='Drink7up@home')\n",
        "    \n",
        "    db_Info = connection.get_server_info()\n",
        "    print(\"\\nConnected to MySQL Server version\", db_Info)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"select database();\")\n",
        "    record = cursor.fetchone()\n",
        "    cursor.fast_executemany = True\n",
        "    with open(files, newline='',  encoding=\"utf8\") as csvfile:\n",
        "      csvdata = csv.reader(csvfile, escapechar=\"\\'\")\n",
        "      #skip the 1st row as it will be header\n",
        "      next(csvdata)\n",
        "      pbar = ProgressBar()\n",
        "      for row in pbar(list(csvdata)):\n",
        "      #Prepare SQL query to INSERT a record into the database.\n",
        "        sql= \"INSERT INTO sector_indice_symbols (company_name,sector, symbol) \\\n",
        "        VALUES ('%s','%s','%s');\" % (row[0], row[1], row[2])\n",
        "        try:\n",
        "          #Execute the SQL command\n",
        "          cursor.execute(sql)\n",
        "          #Commit your changes in the database\n",
        "          connection.commit()\n",
        "        except Error as e:\n",
        "          cprint('-------------------------------','red')\n",
        "          print(\"Error while connecting to MySQL\", e)\n",
        "          cprint('-------------------------------','red')\n",
        "          connection.rollback()\n",
        "          #pbar.next()\n",
        "          pbar.finish()\n",
        "          break\n",
        "  except Error as e:\n",
        "    cprint('-------------------------------','red')\n",
        "    print(\"Error while connecting to MySQL\", e)\n",
        "    cprint('-------------------------------','red')\n",
        "    break\n",
        "cprint(\">>>>>>>>>>\\nDONE\\n>>>>>>>>>>\",'green')\n",
        "if (connection.is_connected()):\n",
        "              cursor.close()\n",
        "              connection.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky43YYVp770w"
      },
      "source": [
        "from nsetools import Nse\n",
        "from pprint import pprint # just for neatness of display\n",
        "nse = Nse()\n",
        "print (nse)\n",
        "#Driver Class for National Stock Exchange (NSE)\n",
        "q = nse.get_quote('infy') # it's ok to use both upper or lower case for codes.\n",
        "pprint(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rgFarplQkfR"
      },
      "source": [
        "pip install nsetools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKwZG2hlmvyD"
      },
      "source": [
        "#################################################\n",
        "#Kini's Out of control Strategy Stocks and FOREX#\n",
        "#################################################\n",
        "SELECT symbol, open, high, low, close, bhavcopydate, tottrdqty,deliverable, deliverable_percent\n",
        "  FROM bhavcopy \n",
        "  where bhavcopydate >= date_sub(curdate(),interval 10 day)\n",
        "  and close between 150 and 1000\n",
        "  and symbol not in (select symbol from banned_symbols)\n",
        " \n",
        "  SELECT symbol, max(high), min(low), stddev(high-low) stddev,avg((high-low)/open) as VFM, \n",
        "  avg(high-low),(avg(high-low)+stddev(high-low)) UCL, high-low, (avg(high-low)-stddev(high-low)) LCL\n",
        "  FROM bhavcopy\n",
        "  where bhavcopydate >= date_sub(curdate(),interval 10 day)\n",
        "  #and symbol='ASAHISONG'\n",
        "  group by symbol;\n",
        "##################################################\n",
        "#Kini's Out of control Strategy Forex #\n",
        "SELECT symbol, open, high, low, close, exp_date, open_int, trd_qty, bhavcopydate\n",
        "  FROM forex\n",
        "  where bhavcopydate >= curdate()-10\n",
        "  and exp_date between '2020-08-01' and '2020-09-01'\n",
        "\n",
        "SELECT symbol, open high, low, close, bhavcopydate, stddev(high-low) stddev, variance(high-low) var, avg(high-low),(avg(high-low)+stddev(high-low)) UCL, high-low, (avg(high-low)-stddev(high-low)) LCL\n",
        "  FROM forex \n",
        "  where bhavcopydate >= curdate()-10\n",
        "  group by symbol\n",
        "##################################################  \n",
        "\n",
        "\n",
        "#############################################\n",
        "#max upside or downside potential of a stock#\n",
        "#############################################\n",
        "SELECT symbol,AVG(uspot_array.uspot) as median_upside_potential_percent\n",
        "FROM (\n",
        "SELECT symbol,(high-open) as uspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM bhavcopy , (SELECT @rownum:=0) r\n",
        "  WHERE symbol = 'geship'\n",
        "  and \n",
        "  bhavcopydate >= date_sub(curdate(),interval 7 day)\n",
        "  ORDER BY uspot\n",
        ") as uspot_array\n",
        "WHERE uspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) )\n",
        "\n",
        "SELECT symbol,AVG(dspot_array.dspot) as median_downside_potential_percent\n",
        "FROM (\n",
        "SELECT symbol,(low-open) as dspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM bhavcopy , (SELECT @rownum:=0) r\n",
        "  WHERE symbol='geship'\n",
        "  and bhavcopydate >= date_sub(curdate(),interval 7 day)\n",
        "  ORDER BY dspot\n",
        ") as dspot_array\n",
        "WHERE dspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) );\n",
        "\n",
        "#############################################\n",
        "#max upside or downside potential of forex#\n",
        "#############################################\n",
        "SELECT symbol,AVG(uspot_array.uspot) as median_upside_potential_in_Paisa\n",
        "FROM (\n",
        "SELECT symbol,(high-open) as uspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM forex , (SELECT @rownum:=0) r\n",
        "  WHERE symbol='EURINR' and exp_date between '2020-08-01' and '2020-09-01'\n",
        "  and bhavcopydate >= date_sub(curdate(),interval 7 day)\n",
        "  ORDER BY uspot\n",
        ") as uspot_array\n",
        "WHERE uspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) );\n",
        "\n",
        "SELECT symbol,AVG(dspot_array.dspot) as median_downside_potential_in_Paisa\n",
        "FROM (\n",
        "SELECT symbol,(low-open) as dspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM forex , (SELECT @rownum:=0) r\n",
        "  WHERE symbol='EURINR'  and exp_date between '2020-08-01' and '2020-09-01'\n",
        "  and bhavcopydate >= date_sub(curdate(),interval 7 day)\n",
        "  ORDER BY dspot\n",
        ") as dspot_array\n",
        "WHERE dspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) );\n",
        "\n",
        "#############################################\n",
        "# volume growth                              #\n",
        "#############################################\n",
        "select symbol,bhavcopydate,\n",
        "    if(@last_entry = 0, 0, round(((tottrdqty - @last_entry) / @last_entry) * 100,2)) \"growth rate\",\n",
        "    @last_entry := tottrdqty\n",
        "    from\n",
        "    (select @last_entry := 0) x,\n",
        "    (select symbol,tottrdqty, bhavcopydate,deliverable_percent\n",
        "    from bhavcopy\n",
        "    where bhavcopydate >=curdate()-7\n",
        "    and symbol='tatachem'\n",
        "    group by bhavcopydate) y;\n",
        "\n",
        "select symbol,bhavcopydate,\n",
        "    if(@last_entry = 0, 0, round(((trd_qty - @last_entry) / @last_entry) * 100,2)) \"growth rate\",\n",
        "    @last_entry := trd_qty\n",
        "    from\n",
        "    (select @last_entry := 0) x,\n",
        "    (select symbol,trd_qty, bhavcopydate\n",
        "    from forex\n",
        "    where bhavcopydate >=curdate()-34\n",
        "    and exp_date between '2020-08-01' and '2020-09-01'\n",
        "    and symbol='GBPINR'\n",
        "    group by bhavcopydate) y;\n",
        "\n",
        "###########################################################  \n",
        "# most volatile stocks in the database in the last 10 days#\n",
        "###########################################################\n",
        "SELECT symbol,CLOSE,`bhavcopydate`, VARIANCE(high-low) var \n",
        "  FROM bhavcopy \n",
        "  where `tottrdqty`> '1000000' and CLOSE between 150 and 500\n",
        "  and `bhavcopydate` >= date_sub(curdate(),interval 10 day)\n",
        "  GROUP BY symbol  \n",
        "ORDER BY `var`  DESC\n",
        "\n",
        "#######################################################################\n",
        "#Max High and low in the last 5 trading sessions , they are KEY levels#\n",
        "#######################################################################\n",
        "\n",
        "select symbol, max(high), min(low) from bhavcopy\n",
        "where bhavcopydate >= date_sub(curdate(),interval 7 day)\n",
        "and symbol in ('PETRONET','GSPL','SBIN','TVSMOTOR','RBLBANK','TATASTEEL')\n",
        "group by symbol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_XYgsBbVhQ"
      },
      "source": [
        "#empty Folder\n",
        "import pandas as pd\n",
        "import glob, os,io\n",
        "\n",
        "base = '/content/drive/My Drive/algotrade/2020'\n",
        "for CleanUp in glob.glob(base+'/Futures/*.*'): # list out the files\n",
        "#  os.remove(CleanUp) #remove files in the folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmbO7G5mnOrJ"
      },
      "source": [
        "import requests, zipfile, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from progressbar import ProgressBar\n",
        "from termcolor import colored, cprint\n",
        "import time\n",
        "\n",
        "#https://honingds.com/blog/pandas-read_csv/#indexcol\n",
        "\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "dmonth={'01':'JAN','02':'FEB','03':'MAR','04':'APR','05':'MAY','06':'JUN','07':'JUL','08':'AUG','09':'SEP','10':'OCT','11':'NOV','12':'DEC'}\n",
        "holiday = ['2020-04-02','2020-04-06','2020-04-10','2020-04-14','2020-05-01','2020-05-25','2020-10-02','2020-11-16','2020-11-30','2020-12-25']\n",
        "# Before running this script , file called bhavcopy_date.txt need to be present in the \"base\" path.\n",
        "# Opening file named bhavcopy_date.txt , it keeps track of the last downloaded date.\n",
        "ltdl = open(base+'bhavcopy_date.txt','r')\n",
        "lastdt = ltdl.read(10)\n",
        "ltdl.close()\n",
        "lastdt = datetime.strptime(lastdt,'%Y-%m-%d')\n",
        "diff, wr = today-lastdt.date(), ''\n",
        "\n",
        "for i in range(1,diff.days+1): #loop through all dates from the last date mentioned in the bhavcopy_date file until today.\n",
        "  nextdt = lastdt+ relativedelta(days=i) #calculate the next day value\n",
        "  #check if the date is a weekend or market holiday so that we can remove that from the loop , bhavcopies are not available for weekends.\n",
        "  if (nextdt.weekday() == 5 or nextdt.weekday() == 6):\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a weekend','grey')\n",
        "  elif nextdt.strftime('%Y-%m-%d') in holiday:\n",
        "    cprint (nextdt.strftime('%Y-%m-%d')+' is a Market Holiday','grey')\n",
        "  else:\n",
        "    d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year #extract day , month and year from  he date\n",
        "    zpath = base+y+'/'+d+'.zip'\n",
        "    if not os.path.isdir(base+y):#if there is no directory already present at the path with the year as a folder then create it\n",
        "      os.mkdir(base+y)\n",
        "      os.mkdir(base+y+'/Index')\n",
        "      os.mkdir(base+y+'/Futures')\n",
        "      os.mkdir(base+y+'/Forex')\n",
        "      os.mkdir(base+y+'/bhavcopy')\n",
        "    for i in range(3): #try to connect to the nseindia url to download the bhavcopy , 7 times , just incase website does not respond etc.\n",
        "      while True:\n",
        "        try:\n",
        "          equities_bhavcopy=requests.get('https://archives.nseindia.com/content/historical/EQUITIES/'+y+'/'+dmonth[m]+'/cm'+d+dmonth[m]+y+'bhav.csv.zip')\n",
        "        except requests.ConnectionError:\n",
        "          print('No connection, retrying')\n",
        "        break\n",
        "    if equities_bhavcopy.status_code!=200:#if the connection is successful\n",
        "      dload=open(zpath, 'wb')\n",
        "      dload.write(equities_bhavcopy.content)\n",
        "      dload.close()\n",
        "      #open the downlaoded bhavcopy and extract it\n",
        "      z = zipfile.ZipFile(zpath, 'r')\n",
        "      z.extractall(base+y+'/bhavcopy')\n",
        "      z.close()\n",
        "      os.remove(zpath)\n",
        "      #reading and storing in 2 dictionaries because we need 2 columns from the MTO file deliverable and %deliverable which is not found in the bhavcopy.\n",
        "      f, deldict = pd.read_csv(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "      f, deldict2 = pd.read_csv(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "      f = f[f['SERIES'] == 'EQ'] #retaining only EQ rows and leaving out bonds,options etc\n",
        "      #cprint('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS', 'red', attrs=['blink'])\n",
        "      print('cm'+d+dmonth[m]+y+'bhav.csv.zip download...SUCCESS')\n",
        "      # a file called mto.dat holds deliverable data, it is useful data to know whether the delivery percentage of a stock has gone up , an indication that long term investors have gone up\n",
        "      for i in range(7): #try to connect to the nseindia url to download the mto file , 7 times , just incase website does not respond etc.\n",
        "        while True:\n",
        "          try:\n",
        "            deliverable = requests.get('https://archives.nseindia.com/archives/equities/mto/MTO_'+d+m+y+'.DAT').text.splitlines()\n",
        "          except requests.ConnectionError:\n",
        "            cprint('No connection, retrying','red')\n",
        "          break\n",
        "      print(''+d+m+y+'.DAT download ...SUCCESS')\n",
        "      del deliverable[:4]\n",
        "      for i in deliverable:\n",
        "          c = i.split(',')\n",
        "          if c[3] == 'EQ' :                \n",
        "              deldict[c[2]] = c[5] #building delivarables dict\n",
        "          if c[3] == 'EQ' :                \n",
        "              deldict2[c[2]] = c[6] #building %delivarables dict\n",
        "      dfdel = pd.DataFrame(list(deldict.items()), columns = ['SYMBOL', 'DELIVERABLE'])\n",
        "      dfdel2 = pd.DataFrame(list(deldict2.items()), columns = ['SYMBOL', '%DELIVERABLE'])\n",
        "      f = f.merge(dfdel, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "      f = f.merge(dfdel2, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "      #write a new csv, bhavcopydate as a column in the csv and get rid of the downloaded file\n",
        "      f['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(f))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "      f = f[['SYMBOL', 'BHAVCOPYDATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'TOTTRDQTY', 'DELIVERABLE','%DELIVERABLE']]\n",
        "      f.to_csv(base+y+'/'+str(nextdt.date())+'.csv', index=False)\n",
        "      os.remove(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv')\n",
        "      print('Merging of deliverable column to EQ bhavcopy...SUCCESS')\n",
        "      \n",
        "      ######################################################################################\n",
        "      # connect to MySQL db in https://johnny.heliohost.org:2083/ UN \n",
        "      # pip install pip install mysql-connector --target=$nb_path pip install mysql-connector\n",
        "      # https://pynative.com/python-mysql-database-connection/\n",
        "      d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "      #check if the path exist and connect to cloud mysql \n",
        "      #if os.path.exists(base+y):\n",
        "      try:\n",
        "        connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                            database='akini_algotrade',\n",
        "                                            user='akini',\n",
        "                                            password='Drink7up@home')\n",
        "        \n",
        "        db_Info = connection.get_server_info()\n",
        "        print(\"\\nConnected to MySQL Server - version\", db_Info)\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(\"select database();\")\n",
        "        record = cursor.fetchone()\n",
        "        print(\"\\nIngesting EQUITIES bhavcopy \"+'/'+y+'-'+m+'-'+d+'.csv'+\" into DB....:\", record)\n",
        "        cursor.fast_executemany = True\n",
        "        with open(base+y+'/bhavcopy/'+y+'-'+m+'-'+d+'.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "          csvdata = csv.reader(csvfile)\n",
        "          #skip the 1st row as it will be header\n",
        "          next(csvdata)\n",
        "          pbar = ProgressBar()\n",
        "          for row in pbar(list(csvdata)):\n",
        "            # Prepare SQL query to INSERT a record into the database.\n",
        "            sql_stocks = \"INSERT INTO bhavcopy (symbol, bhavcopydate, open, high, low, close, tottrdqty, deliverable, deliverable_percent) \\\n",
        "            VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8])\n",
        "            #print(sql)\n",
        "            try:\n",
        "              #Execute the SQL command\n",
        "              cursor.execute(sql_stocks)\n",
        "              #Commit your changes in the database\n",
        "              connection.commit()\n",
        "            except Error as e:\n",
        "              cprint('-------------------------------','red')\n",
        "              print(\"Error while connecting to MySQL\", e)\n",
        "              cprint('-------------------------------','red')\n",
        "              connection.rollback()\n",
        "              #pbar.next()\n",
        "              pbar.finish()\n",
        "              break\n",
        "      except Error as e:\n",
        "        cprint('-------------------------------','red')\n",
        "        print(\"Error while connecting to MySQL\", e)\n",
        "        cprint('-------------------------------','red')\n",
        "        break\n",
        "      finally:\n",
        "        if (connection.is_connected()):\n",
        "          cursor.close()\n",
        "          connection.close()\n",
        "    #now lets work with the FOREX\n",
        "    for i in range(2): #try to connect to the nseindia url to download the bhavcopy , 3 times , just incase website does not respond etc.\n",
        "      #while True:\n",
        "      try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',}#Sometimes requests from requests.get() gets blocked by server, so solution is to make the server think the request is coming from a web browser\n",
        "        forex = requests.get('https://archives.nseindia.com/archives/cd/mkt_act/cd'+d+m+y+'.zip', headers=headers, timeout=5) #get the zip file from nseindia\n",
        "      except requests.exceptions.Timeout as error:\n",
        "        cprint('ERROR!!! 404 filenotfound cd'+d+m+y+'.zip..skipping','red')\n",
        "        break\n",
        "      except requests.ConnectionError:\n",
        "        cprint('No connection.. retrying','red')\n",
        "        break\n",
        "    if forex.status_code!=200:\n",
        "        fx=open(zpath, 'wb') #open the zip file under a temp location called zpath that is defined above\n",
        "        fx.write(forex.content)\n",
        "        fx.close()\n",
        "        z, wr = zipfile.ZipFile(zpath,'r'), nextdt.date()\n",
        "        z.extractall(base+y+'/Forex') #extract the contents of the zip file to a location called forex , it extracts some 6 csv files we want only 1\n",
        "        z.close()\n",
        "        os.remove(zpath) #remove the zip file\n",
        "        if os.path.exists(base+y+'/Forex/cf'+d+m+y+'.csv'):\n",
        "          def trim(dataset): #Definition for strippping whitespace\n",
        "              trim = lambda x: x.strip() if type(x) is str else x\n",
        "              return dataset.applymap(trim)\n",
        "          # making dataframe from csv file\n",
        "          data = trim(pd.read_csv(base+y+'/Forex/cf'+d+m+y+'.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "          data = data[data['INSTRUMENT'] == 'FUTCUR'] #retaining only FUTCUR rows and leaving out other rows\n",
        "          data[['INSTRUMENT', 'SYMBOL    ', 'EXP_DATE  ', 'OPEN_PRICE ', 'HI_PRICE   ', 'LO_PRICE   ', 'CLOSE_PRICE','OPEN_INT*      ','TRD_VAL           ','TRD_QTY          ','NO_OF_CONT       ','NO_OF_TRADE      ']]#the useless file has spaces in the heading.\n",
        "          data = data.rename(columns={'SYMBOL    ':'SYMBOL', 'EXP_DATE  ':'EXP_DATE', 'OPEN_PRICE ':'OPEN', 'HI_PRICE   ':'HIGH', 'LO_PRICE   ':'LOW', 'CLOSE_PRICE':'CLOSE','OPEN_INT*      ':'OPEN_INT','TRD_VAL           ':'TRD_VAL','TRD_QTY          ':'TRD_QTY','NO_OF_CONT       ':'NO_OF_CONT','NO_OF_TRADE      ':'NO_OF_TRADE'}) #rename some of the columns to something that is easier to underastand\n",
        "          data['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(data))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "          data['EXP_DATE'] = pd.to_datetime(data['EXP_DATE'], format = '%d/%m/%Y') #the exp_date column is in format dd/mm/yyy convert it to yyyy-mm-dd\n",
        "          data.to_csv(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', index=False) #write the changes above to a new file and add an _forex to the csv file\n",
        "          os.remove(base+y+'/Forex/cf'+d+m+y+'.csv')\n",
        "          print('Forex bhavcopy '+ str(nextdt.date())+'_forex.csv create...SUCCESS')\n",
        "          #print(data)\n",
        "          ######################################################################################\n",
        "          #cleanup all the useless files that get extracted from the currency futures bhavcopy\n",
        "          for CleanUp in glob.glob(base+y+'/Forex/*.*'): # list out the files\n",
        "            if not re.match(\".+forex+\",CleanUp): #if the list found above contains \"forex\" then dont do anything, else delete\n",
        "              os.remove(CleanUp) #remove files in the folder\n",
        "            \n",
        "          try:\n",
        "            connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                                database='akini_algotrade',\n",
        "                                                user='akini',\n",
        "                                                password='Drink7up@home')\n",
        "            \n",
        "            db_Info = connection.get_server_info()\n",
        "            print(\"\\nConnected to MySQL Server version\", db_Info)\n",
        "            #print(\">>>>>>>>>>>>>>>\\n\")\n",
        "            cursor = connection.cursor()\n",
        "            cursor.execute(\"select database();\")\n",
        "            record = cursor.fetchone()\n",
        "            print(\"\\nIngesting FOREX bhavcopy \"+'/'+y+'-'+m+'-'+d+'_forex.csv'+\" into DB....:\", record)\n",
        "            #print(\">>>>>>>>>>>>>>>\\n\")\n",
        "            cursor.fast_executemany = True\n",
        "            with open(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', newline='',  encoding=\"utf8\") as csvfile_forex:\n",
        "              csvdata_forex = csv.reader(csvfile_forex)\n",
        "              #skip the 1st row as it will be header\n",
        "              next(csvdata_forex)\n",
        "              pbar = ProgressBar()\n",
        "              for row in pbar(list(csvdata_forex)):\n",
        "              #Prepare SQL query to INSERT a record into the database.\n",
        "                sql_forex = \"INSERT INTO forex (symbol, exp_date, open, high, low, close, open_int, trd_val, trd_qty, no_of_cont, no_of_trade, bhavcopydate) \\\n",
        "                VALUES ('%s','%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s', '%s','%s');\" % (row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11], row[12])\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_forex)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "          except Error as e:\n",
        "            cprint('-------------------------------','red')\n",
        "            print(\"Error while connecting to MySQL\", e)\n",
        "            cprint('-------------------------------','red')\n",
        "            break\n",
        "          finally:\n",
        "            # create cursor\n",
        "            cursor=connection.cursor()\n",
        "            csvfile = requests.get('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/edit#gid=0')\n",
        "            print(csvfile.status_code)\n",
        "            print('Updating Banned Symbols table')\n",
        "            # Insert DataFrame recrds one by one.\n",
        "            df=pd.read_csv('https://docs.google.com/spreadsheets/d/1ZTyh6GiHTwA1d-ApYdn5iCmRiBLZoAtwigS7VyLUk_Y/export?format=csv&gid=0', usecols=[\"Stocks not allowed for MIS\"])\n",
        "            pbar = ProgressBar()\n",
        "            #Prepare SQL query to INSERT a record into the database.\n",
        "            if csvfile.status_code != 200:\n",
        "              #truncate banned symbols table\n",
        "              sql_truncate_table = \"truncate banned_symbols;\"\n",
        "              cursor.execute(sql_truncate_table)\n",
        "              connection.commit()\n",
        "              time.sleep(10)\n",
        "              for i,row in pbar(df.iterrows()):\n",
        "                sql_banned_symbols = \"INSERT INTO banned_symbols (symbol) VALUES ('%s');\" % (row[0])\n",
        "                try:\n",
        "                  #Execute the SQL command\n",
        "                  cursor.execute(sql_banned_symbols)\n",
        "                  #Commit your changes in the database\n",
        "                  connection.commit()\n",
        "                except Error as e:\n",
        "                  cprint('-------------------------------','red')\n",
        "                  print(\"Error while connecting to MySQL\", e)\n",
        "                  cprint('-------------------------------','red')\n",
        "                  connection.rollback()\n",
        "                  #pbar.next()\n",
        "                  pbar.finish()\n",
        "                  break\n",
        "                  connection.close()\n",
        "if (connection.is_connected()):\n",
        "  cursor.close()\n",
        "  connection.close()\n",
        "cprint(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\nDONE - All imports complete\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\",'green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYFOTK0Ez2gF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}