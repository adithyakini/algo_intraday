{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bhavcopy_nseindia2mysql.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wpeVd-eds_hCQW83PZg9epPy9KzIGStl",
      "authorship_tag": "ABX9TyNJmlKqIWXDPXK8FFEY9yxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyakini/algo_intraday/blob/master/bhavcopy_nseindia2mysql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGXTBbodmqb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1a4b513b-0ed1-48a7-b2dd-695c4dc26a1e"
      },
      "source": [
        "pip install mysql.connector"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/8c/83/a1/f8b6d4bb1bd6208bbde1608bbfa7557504bed9eaf2ecf8c175/mysql_connector-2.2.9-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: mysql.connector\n",
            "Successfully installed mysql.connector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcnb-noyOvgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9eKJeMSoVuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "0466ff83-89aa-4606-83f4-71aecb083208"
      },
      "source": [
        "#######################################################\n",
        "#forex read csv , filter on currency only, trim cells #\n",
        "#to remove whitespace, add to DF, rename columns      #\n",
        "#create a new forex csv and delete the old one        #\n",
        "#######################################################\n",
        "def trim(dataset): #Definition for strippping whitespace\n",
        "    trim = lambda x: x.strip() if type(x) is str else x\n",
        "    return dataset.applymap(trim)\n",
        "# making dataframe from csv file  \n",
        "data = trim(pd.read_csv(base+y+'/Forex/'+y+'-'+m+'-'+d+'_forex.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "#data = data[data['INSTRUMENT'] == 'FUTCUR'] #retaining only FUTCUR rows and leaving out other rows\n",
        "#data[['INSTRUMENT', 'SYMBOL    ', 'EXP_DATE  ', 'OPEN_PRICE ', 'HI_PRICE   ', 'LO_PRICE   ', 'CLOSE_PRICE','OPEN_INT*      ','TRD_VAL           ','TRD_QTY          ','NO_OF_CONT       ','NO_OF_TRADE      ']]\n",
        "#data = data.rename(columns={'OPEN_PRICE ':'OPEN', 'HI_PRICE   ':'HIGH', 'LO_PRICE   ':'LOW', 'CLOSE_PRICE':'CLOSE','OPEN_INT*      ':'OPEN_INT'})\n",
        "data [3]\n",
        "#ts = data[3]\n",
        "#ts = datetime.strptime(ts, '%d/%m/%Y').strftime(\"%Y-%m-%d\")\n",
        "#if ts != \"\":\n",
        "#    data[3] = ts # this is what you miss\n",
        "#    writer.writerow(data)\n",
        "#data.columns.values[3] = 'OPEN'\n",
        "data\n",
        "#data.to_csv(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', index=False)\n",
        "#os.remove(base+y+'/Forex/cf'+d+m+y+'.csv')\n",
        "#######################################################"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-c7c786f0beb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#data[['INSTRUMENT', 'SYMBOL    ', 'EXP_DATE  ', 'OPEN_PRICE ', 'HI_PRICE   ', 'LO_PRICE   ', 'CLOSE_PRICE','OPEN_INT*      ','TRD_VAL           ','TRD_QTY          ','NO_OF_CONT       ','NO_OF_TRADE      ']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#data = data.rename(columns={'OPEN_PRICE ':'OPEN', 'HI_PRICE   ':'HIGH', 'LO_PRICE   ':'LOW', 'CLOSE_PRICE':'CLOSE','OPEN_INT*      ':'OPEN_INT'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#ts = data[3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#ts = datetime.strptime(ts, '%d/%m/%Y').strftime(\"%Y-%m-%d\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky43YYVp770w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rgFarplQkfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQ7pBunZZyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e658122e-5ebd-46bc-cf5b-4a3653e2a080"
      },
      "source": [
        "import requests, zipfile, os, io, pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from progressbar import ProgressBar\n",
        "#set the path to where the bhavcopies will be downloaded\n",
        "base = '/content/drive/My Drive/algotrade/'\n",
        "today = datetime.today().date()\n",
        "dmonth={'01':'JAN','02':'FEB','03':'MAR','04':'APR','05':'MAY','06':'JUN','07':'JUL','08':'AUG','09':'SEP','10':'OCT','11':'NOV','12':'DEC'}\n",
        "holiday = ['2020-05-01', '2020-05-25', '2020-10-02', '2020-11-16', '2020-11-30', '2020-12-25']\n",
        "\n",
        "# Before running this script , file called bhavcopy_date.txt need to be present in the \"base\" path.\n",
        "# Opening file named bhavcopy_date.txt , it keeps track of the last downloaded date.\n",
        "ltdl = open(base+'bhavcopy_date.txt','r')\n",
        "lastdt = ltdl.read(10)\n",
        "ltdl.close()\n",
        "lastdt = datetime.strptime(lastdt,'%Y-%m-%d')\n",
        "diff, wr = today-lastdt.date(), ''\n",
        "\n",
        "for i in range(1,diff.days+1): #loop through all dates from the last date mentioned in the bhavcopy_date file until today.\n",
        "    nextdt = lastdt+ relativedelta(days=i) #calculate the next day value\n",
        "    #check if the date is a weekend or market holiday so that we can remove that from the loop , bhavcopies are not available for weekends.\n",
        "    if (nextdt.weekday() == 5 or nextdt.weekday() == 6):\n",
        "      print (nextdt.strftime('%Y-%m-%d')+\" is a weekend\")\n",
        "    elif nextdt.strftime('%Y-%m-%d') in holiday:\n",
        "      print (nextdt.strftime('%Y-%m-%d')+\" is a Market Holiday\")\n",
        "    else:\n",
        "      d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year #extract day , month and year from  he date\n",
        "      if not os.path.isdir(base+y):#if there is no directory already present at the path with the year as a folder then create it\n",
        "        os.mkdir(base+y)\n",
        "        os.mkdir(base+y+'/Index')\n",
        "        os.mkdir(base+y+'/Futures')\n",
        "        os.mkdir(base+y+'/Forex')\n",
        "      zpath = base+y+'/'+d+'.zip'\n",
        "      \n",
        "      for i in range(7): #try to connect to the nseindia url to download the bhavcopy , 7 times , just incase website does not respond etc.\n",
        "        while True:\n",
        "          try:\n",
        "            equities_bhavcopy=requests.get('https://archives.nseindia.com/content/historical/EQUITIES/'+y+'/'+dmonth[m]+'/cm'+d+dmonth[m]+y+'bhav.csv.zip')\n",
        "          except requests.ConnectionError:\n",
        "            print('No connection, retrying')\n",
        "          break\n",
        "      \n",
        "      if equities_bhavcopy.status_code==200:#if the connection is successful\n",
        "        dload=open(zpath, 'wb') \n",
        "        dload.write(equities_bhavcopy.content)\n",
        "        dload.close()\n",
        "        #open the downlaoded bhavcopy and extract it\n",
        "        z = zipfile.ZipFile(zpath, 'r')\n",
        "        z.extractall(base+y+'/')\n",
        "        z.close()\n",
        "        os.remove(zpath)\n",
        "        #reading and storing in 2 dictionaries because we need 2 columns from the MTO file deliverable and %deliverable which is not found in the bhavcopy.\n",
        "        f, deldict = pd.read_csv(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "        f, deldict2 = pd.read_csv(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv'), {}  #reading the raw dl-ed bhav file\n",
        "        f = f[f['SERIES'] == 'EQ'] #retaining only EQ rows and leaving out bonds,options etc\n",
        "\n",
        "        # a file called mto.dat holds deliverable data, it is useful data to know whether the delivery percentage of a stock has gone up , an indication that long term investors have gone up\n",
        "        deliverable = requests.get('https://archives.nseindia.com/archives/equities/mto/MTO_'+d+m+y+'.DAT').text.splitlines()\n",
        "        del deliverable[:4]\n",
        "        for i in deliverable:\n",
        "            c = i.split(',')\n",
        "            if c[3] == 'EQ' :                \n",
        "                deldict[c[2]] = c[5] #building delivarables dict\n",
        "            if c[3] == 'EQ' :                \n",
        "                deldict2[c[2]] = c[6] #building %delivarables dict\n",
        "        dfdel = pd.DataFrame(list(deldict.items()), columns = ['SYMBOL', 'DELIVERABLE'])\n",
        "        dfdel2 = pd.DataFrame(list(deldict2.items()), columns = ['SYMBOL', '%DELIVERABLE'])\n",
        "        f = f.merge(dfdel, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "        f = f.merge(dfdel2, on='SYMBOL', how='left')      #left merge of delivarables here\n",
        "        \n",
        "        f=f.append(indx, ignore_index=True)\n",
        "        #write a new csv, bhavcopydate as a column in the csv and get rid of the downloaded file\n",
        "        f['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(f))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "        f = f[['SYMBOL', 'BHAVCOPYDATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'TOTTRDQTY', 'DELIVERABLE','%DELIVERABLE']]\n",
        "        f.to_csv(base+y+'/'+str(nextdt.date())+'.csv', index=False)\n",
        "        os.remove(base+y+'/cm'+d+dmonth[m]+y+'bhav.csv')\n",
        "\n",
        "        #sometimes nse doesnt give the index file, so the if condition\n",
        "        indices = requests.get('https://archives.nseindia.com/content/indices/ind_close_all_'+d+m+y+'.csv').content\n",
        "        if len(indices)>300:\n",
        "          indx = pd.read_csv(io.StringIO(indices.decode('utf-8'))) #reading content of indices csv and storing in DataFrame using io module\n",
        "          indx.to_csv(base+y+'/Index/Indices'+ str(nextdt.date())+'.csv', index=False)\n",
        "          indx[['Index Name', 'Index Date', 'Open Index Value', 'High Index Value', 'Low Index Value', 'Closing Index Value', 'Volume']]\n",
        "          indx = indx.rename(columns={'Index Name' : 'SYMBOL', 'Index Date' : 'BHAVCOPYDATE', 'Open Index Value' : 'OPEN', 'High Index Value' : 'HIGH', 'Low Index Value' : 'LOW', 'Closing Index Value' : 'CLOSE', 'Volume' : 'TOTTRDQTY'})\n",
        "\n",
        "        ######################################################################################\n",
        "        #futures download the zip from nseindia , and extract not inserting into db yet      #\n",
        "        ######################################################################################\n",
        "        for i in range(7): #try to connect to the nseindia url to download the bhavcopy , 7 times , just incase website does not respond etc.\n",
        "          while True:\n",
        "            try:\n",
        "              futures = requests.get('https://archives.nseindia.com/content/historical/DERIVATIVES/'+y+'/'+dmonth[m]+'/fo'+d+dmonth[m]+y+'bhav.csv.zip')\n",
        "            except requests.ConnectionError:\n",
        "              print('No connection, retrying')\n",
        "            break\n",
        "        fo = open(zpath, 'wb')\n",
        "        fo.write(futures.content)\n",
        "        fo.close()\n",
        "        z, wr = zipfile.ZipFile(zpath,'r'), nextdt.date()\n",
        "        z.extractall(base+y+'/Futures')\n",
        "        z.close()\n",
        "        os.remove(zpath)\n",
        "        ######################################################################################\n",
        "        #forex download the zip from nseindia , read csv , filter on currency only, trim     #\n",
        "        #to remove whitespace, add to DF, rename columns create a new forex csv and delete   #\n",
        "        #the old one.                                                                        #\n",
        "        ######################################################################################\n",
        "        forex=requests.get('https://archives.nseindia.com/archives/cd/mkt_act/cd'+d+m+y+'.zip') #get the zip file from nseindia\n",
        "        fx=open(zpath, 'wb') #open the zip file under a temp location called zpath that is defined above\n",
        "        fx.write(forex.content) \n",
        "        fx.close()\n",
        "        z, wr = zipfile.ZipFile(zpath,'r'), nextdt.date()\n",
        "        z.extractall(base+y+'/Forex') #extract the contents of the zip file to a location called forex , it extracts some 6 csv files we want only 1\n",
        "        z.close()\n",
        "        os.remove(zpath) #remove the zip file\n",
        "        def trim(dataset): #Definition for strippping whitespace\n",
        "            trim = lambda x: x.strip() if type(x) is str else x\n",
        "            return dataset.applymap(trim)\n",
        "        # making dataframe from csv file\n",
        "        data = trim(pd.read_csv(base+y+'/Forex/cf'+d+m+y+'.csv'))  #reading the raw dl-ed file and trimming the trailing spaces with trim()\n",
        "        data = data[data['INSTRUMENT'] == 'FUTCUR'] #retaining only FUTCUR rows and leaving out other rows\n",
        "        data[['INSTRUMENT', 'SYMBOL    ', 'EXP_DATE  ', 'OPEN_PRICE ', 'HI_PRICE   ', 'LO_PRICE   ', 'CLOSE_PRICE','OPEN_INT*      ','TRD_VAL           ','TRD_QTY          ','NO_OF_CONT       ','NO_OF_TRADE      ']]#the useless file has spaces in the heading.\n",
        "        data = data.rename(columns={'OPEN_PRICE ':'OPEN', 'HI_PRICE   ':'HIGH', 'LO_PRICE   ':'LOW', 'CLOSE_PRICE':'CLOSE','OPEN_INT*      ':'OPEN_INT'}) #rename some of the columns to something that is easier to underastand\n",
        "        data['BHAVCOPYDATE'] = pd.Series(str(nextdt.date().strftime('%Y-%m-%d')) for _ in range(len(data))) #add a column called bhavcopydate and then fill it with the bhavecopy date\n",
        "        data.to_csv(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', index=False) #write the changes above to a new file and add an _forex to the csv file\n",
        "        os.remove(base+y+'/Forex/cf'+d+m+y+'.csv')\n",
        "        ######################################################################################\n",
        "        #cleanup all the useless files that get extracted from the currency futures bhavcopy\n",
        "        for CleanUp in glob.glob(base+y+'/Forex/*.*'): # list out the files\n",
        "          if not re.match(\".+forex+\",CleanUp): #if the list found above contains \"forex\" then dont do anything, else delete\n",
        "            os.remove(CleanUp) #remove files in the folder\n",
        "        print('>>>>>>>>>>>>>>>\\nDownloaded bhavcopy files...now inserting into DB')\n",
        "        ######################################################################################\n",
        "        # connect to MySQL db in https://johnny.heliohost.org:2083/ UN \n",
        "        # pip install pip install mysql-connector --target=$nb_path pip install mysql-connector\n",
        "        # https://pynative.com/python-mysql-database-connection/\n",
        "\n",
        "        d, m, y = '%02d' % nextdt.day, '%02d' % nextdt.month, '%02d' % nextdt.year\n",
        "\n",
        "        #check if the path exist and connect to cloud mysql \n",
        "        #if os.path.exists(base+y):\n",
        "        try:\n",
        "          connection = mysql.connector.connect(host='johnny.heliohost.org',\n",
        "                                              database='akini_algotrade',\n",
        "                                              user='akini',\n",
        "                                              password='Drink7up@home')\n",
        "          \n",
        "          db_Info = connection.get_server_info()\n",
        "          print(\">>>>>>>>>>>>>>>\\nConnected to MySQL Server version\", db_Info)\n",
        "          print(\">>>>>>>>>>>>>>>\\n\")\n",
        "          cursor = connection.cursor()\n",
        "          cursor.execute(\"select database();\")\n",
        "          record = cursor.fetchone()\n",
        "          print(\"\\n>>>>>>>>>>>>>>>\\nIngesting EQUITIES bhavcopy data from \"+'/'+y+'-'+m+'-'+d+'.csv'+\" into DB....:\", record)\n",
        "          print(\">>>>>>>>>>>>>>>\\n\")\n",
        "          cursor.fast_executemany = True\n",
        "#          with open(base+y+'/'+y+'-'+m+'-'+d+'.csv', newline='',  encoding=\"utf8\") as csvfile:\n",
        "#            csvdata = csv.reader(csvfile)\n",
        "#            #skip the 1st row as it will be header\n",
        "#            next(csvdata)\n",
        "#            pbar = ProgressBar()\n",
        "#            for row in pbar(list(csvdata)):\n",
        "#              # Prepare SQL query to INSERT a record into the database.\n",
        "              #sql_stocks = \"INSERT INTO bhavcopy (symbol, bhavcopydate, open, high, low, close, tottrdqty, deliverable, deliverable_percent) \\\n",
        "              #VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8])\n",
        "              #print(sql)\n",
        "#              try:\n",
        "#                #Execute the SQL command\n",
        "#                # cursor.execute(sql_stocks)\n",
        "#                #Commit your changes in the database\n",
        "#                connection.commit()\n",
        "#              except Error as e:\n",
        "#                print(\"Error while connecting to MySQL\", e)\n",
        "#                connection.rollback()\n",
        "#                #pbar.next()\n",
        "#                pbar.finish()\n",
        "            #now insert the forex bhavcopy into the db\n",
        "          print(\"\\n>>>>>>>>>>>>>>>\\nIngesting FOREX bhavcopy data from \"+'/'+y+'-'+m+'-'+d+'_forex.csv'+\" into DB....:\", record)\n",
        "          print(\">>>>>>>>>>>>>>>\\n\")\n",
        "          with open(base+y+'/Forex/'+ str(nextdt.date())+'_forex.csv', newline='',  encoding=\"utf8\") as csvfile_forex:\n",
        "            csvdata_forex = csv.reader(csvfile_forex)\n",
        "            #skip the 1st row as it will be header\n",
        "            next(csvdata_forex)\n",
        "            pbar = ProgressBar()\n",
        "            for row in pbar(list(csvdata_forex)):\n",
        "              \n",
        "              # Prepare SQL query to INSERT a record into the database.\n",
        "              sql_forex = \"INSERT INTO forex (symbol, exp_date, open, high, low, close, open_int, trd_val, trd_qty, no_of_cont, no_of_trade, bhavcopydate) \\\n",
        "              VALUES ('%s', '%s','%s', '%s','%s', '%s', '%s', '%s','%s','%s', '%s','%s');\" % (row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11])\n",
        "              try:\n",
        "                #Execute the SQL command\n",
        "                cursor.execute(sql_forex)\n",
        "                #Commit your changes in the database\n",
        "                connection.commit()\n",
        "              except Error as e:\n",
        "                print(\"Error while connecting to MySQL\", e)\n",
        "                connection.rollback()\n",
        "                #pbar.next()\n",
        "                pbar.finish()\n",
        "        except Error as e:\n",
        "          print(\"Error while connecting to MySQL\", e)\n",
        "        finally:\n",
        "          if (connection.is_connected()):\n",
        "            cursor.close()\n",
        "            connection.close()   \n",
        "print(\">>>>>>>>>>>>>>>\\nDONE - All imports complete\\n>>>>>>>>>>>>>>>\")\n",
        "\n",
        "#bhavcopydate and exp_date are not working in the db check whats wrong\n",
        "\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>\n",
            "Downloaded bhavcopy files...now inserting into DB\n",
            ">>>>>>>>>>>>>>>\n",
            "Connected to MySQL Server version 5.7.30\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 27) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting EQUITIES bhavcopy data from /2020-07-14.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting FOREX bhavcopy data from /2020-07-14_forex.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (27 of 27) |########################| Elapsed Time: 0:00:18 Time:  0:00:18\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>\n",
            "Downloaded bhavcopy files...now inserting into DB\n",
            ">>>>>>>>>>>>>>>\n",
            "Connected to MySQL Server version 5.7.30\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 27) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting EQUITIES bhavcopy data from /2020-07-15.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting FOREX bhavcopy data from /2020-07-15_forex.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (27 of 27) |########################| Elapsed Time: 0:00:11 Time:  0:00:11\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>\n",
            "Downloaded bhavcopy files...now inserting into DB\n",
            ">>>>>>>>>>>>>>>\n",
            "Connected to MySQL Server version 5.7.30\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 25) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting EQUITIES bhavcopy data from /2020-07-16.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting FOREX bhavcopy data from /2020-07-16_forex.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (25 of 25) |########################| Elapsed Time: 0:00:17 Time:  0:00:17\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>\n",
            "Downloaded bhavcopy files...now inserting into DB\n",
            ">>>>>>>>>>>>>>>\n",
            "Connected to MySQL Server version 5.7.30\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3% (1 of 29) |                         | Elapsed Time: 0:00:00 ETA:   0:00:05"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting EQUITIES bhavcopy data from /2020-07-17.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            ">>>>>>>>>>>>>>>\n",
            "Ingesting FOREX bhavcopy data from /2020-07-17_forex.csv into DB....: ('akini_algotrade',)\n",
            ">>>>>>>>>>>>>>>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (29 of 29) |########################| Elapsed Time: 0:00:06 Time:  0:00:06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>\n",
            "DONE - All imports complete\n",
            ">>>>>>>>>>>>>>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPr209foM-c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKwZG2hlmvyD",
        "colab_type": "text"
      },
      "source": [
        "# most volatile stocks in the database in the last 1 week\n",
        "SELECT symbol,CLOSE,`bhavcopydate`, VARIANCE(high-low) var \n",
        "  FROM bhavcopy \n",
        "  where `tottrdqty`> '10000000' and CLOSE between 100 and 200\n",
        "  and `bhavcopydate` > CURRENT_DATE-5\n",
        "  GROUP BY symbol  \n",
        "ORDER BY `var`  DESC\n",
        "\n",
        "# most upside or downside potential of a stock\n",
        "SELECT AVG(uspot_array.uspot) as median_upside_potential\n",
        "FROM (\n",
        "SELECT ((high-open)/open)*100 as uspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM bhavcopy , (SELECT @rownum:=0) r\n",
        "  WHERE symbol='granules'\n",
        "  ORDER BY uspot\n",
        ") as uspot_array\n",
        "WHERE uspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) );\n",
        "\n",
        "SELECT AVG(dspot_array.dspot) as median_downside_potential\n",
        "FROM (\n",
        "SELECT ((low-open)/open)*100 as dspot, @rownum:=@rownum+1 as `row_number`, @total_rows:=@rownum\n",
        "  FROM bhavcopy , (SELECT @rownum:=0) r\n",
        "  WHERE symbol='granules'\n",
        "  ORDER BY dspot\n",
        ") as dspot_array\n",
        "WHERE dspot_array.row_number IN ( FLOOR((@total_rows+1)/2), FLOOR((@total_rows+2)/2) );"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_XYgsBbVhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}